{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cgradu/DSM/blob/main/Lab_02_Convolutional_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Networks"
      ],
      "metadata": {
        "id": "irfxF57aLV_6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this lab we will explore Convolutional Neural Networks for image classification."
      ],
      "metadata": {
        "id": "HD0sDSA5Lahb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolution Operation"
      ],
      "metadata": {
        "id": "UZozcofHLpMb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/0/04/Convolution_arithmetic_-_Padding_strides.gif?20190413174630\" />"
      ],
      "metadata": {
        "id": "J_4KfNKRL_LO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The problems of using fully connected neural networks for processing images:\n",
        "- The image needs to be transformed into a vector - spatial information is discarded;\n",
        "- Each neuron of the first layer is connected to all pixel values of the image - a lot of parameters for a single layer"
      ],
      "metadata": {
        "id": "wZEFu1dqMkKs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main ideas of CNN:\n",
        "- Each neuron looks only at a local window of the input\n",
        "- You slide a small kernel (set of weights) across the whole image - the same weights are re-used for the whole image\n",
        "- You utilize different kernels for extracting different features as each set of weights is looking for something else"
      ],
      "metadata": {
        "id": "4qtRy26oN05Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following visualization:\n",
        "- The green matrix - input\n",
        "- Yellow matrix that slides across the image - the kernel (set of weights) - trainable\n",
        "- The result - feature map\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1052/0*jLoqqFsO-52KHTn9.gif\"/>"
      ],
      "metadata": {
        "id": "Ubx61vN5PC8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The kernel has the same channel size as the input\n",
        "- The input does not have to be an image - it just has to be an image-like structure\n",
        "- We stack the resulting feature maps of each kernel convolution into an image-like structure - so that we can apply other convolutions to it and extract more meaningful features\n",
        "\n",
        "<img src=\"https://animatedai.github.io/media/convolution-animation-3x3-kernel.gif\" width=700/>"
      ],
      "metadata": {
        "id": "BA8aHsczRJ8F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pooling"
      ],
      "metadata": {
        "id": "yhdaUK56SasY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We utilize **pooling layers** to downsample the spatial size (height and width) while keeping the relevant information.\n",
        "- Usually there are no trainable parameters in this layer - it is just used for **efficiency**.\n",
        "\n",
        "<img src=\"https://wikidocs.net/images/page/164835/Fig_03.png\" width=500/>"
      ],
      "metadata": {
        "id": "VKmlVCfFSoPl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usual types of pooling:\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1200/0*lIcR0gOMK0xzTr6_.png\" />\n",
        "\n",
        "You can check out the types of pooling layers available in PyTorch [here](https://pytorch.org/docs/stable/nn.html#pooling-layers)"
      ],
      "metadata": {
        "id": "aQ74B-rFTKs0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolutional Neural Network\n",
        "\n",
        "- A CNN is composed of convolution layers + activation functions + some pooling layers\n",
        "- For classification a fully connected network is utilized - its input is the last set of feature maps obtained by the CNN layers\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*vkQ0hXDaQv57sALXAJquxA.jpeg\" />"
      ],
      "metadata": {
        "id": "D8GGOeskVPW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNNs in PyTorch"
      ],
      "metadata": {
        "id": "aHVLM77KWP6n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Necessary imports for this lab"
      ],
      "metadata": {
        "id": "jSsFemVMWZXx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2xKWkeGLKWv"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the CIFAR10 dataset which contains 32x32 images of 10 different classes. You can read about the dataset [here](https://www.cs.toronto.edu/~kriz/cifar.html)"
      ],
      "metadata": {
        "id": "cEq2mj8uWjUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO for Exercise 2 - Add more transforms - data augmentations (only for training samples)\n",
        "train_transform = transforms.Compose(\n",
        "    [\n",
        "      # For Exercise 2 - Add image transforms here before ToTensor()\n",
        "\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "test_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "trainset = datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=train_transform)\n",
        "\n",
        "# The batch size is the number of images the model processes in parallel\n",
        "# We use shuffle for training as we don't want the model to see the images in the same order\n",
        "trainloader = DataLoader(trainset, batch_size=256,\n",
        "                                          shuffle=True)\n",
        "\n",
        "testset = datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=test_transform)\n",
        "\n",
        "# For testing we don't have to shuffle the data\n",
        "testloader = DataLoader(testset, batch_size=256,\n",
        "                                         shuffle=False)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "id": "gBCaHpDGWssl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We plot some images from the dataset in order to get an idea of how they look."
      ],
      "metadata": {
        "id": "-ntIMD9_kxUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# function to show an image\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images[:8]))\n",
        "\n",
        "# print labels\n",
        "print(' '.join('%8s' % classes[labels[j]] for j in range(8)))"
      ],
      "metadata": {
        "id": "0Rdg6zBWkzF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define the device on which we will train the models. For this lab you should have GPU execution enabled. Training the models on cpu will take too much time.\n",
        "\n",
        "If the models still take too much time to train, you can try to increase the batch size value in the dataloaders (if you increase it too much you might get \"Out of memory error\")."
      ],
      "metadata": {
        "id": "zeA6Lf5flDxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "zfmAQi_RlHUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define a training epoch in which we iterate through all the data in the training set and update the model.\n",
        "\n",
        "A model is usually trained for multiple epochs - it sees the training data multiple times."
      ],
      "metadata": {
        "id": "APMKuS-4lNCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, dataloader, device, optimizer, criterion, epoch):\n",
        "    # We set the model to be in training mode\n",
        "    model.train()\n",
        "\n",
        "    total_train_loss = 0.0\n",
        "    dataset_size = 0\n",
        "\n",
        "    # This is only for showing the progress bar\n",
        "    bar = tqdm(enumerate(dataloader), total=len(dataloader), colour='cyan', file=sys.stdout)\n",
        "\n",
        "    # We iterate through all batches - 1 step is 1 batch of batch_size images\n",
        "    for step, (images, labels) in bar:\n",
        "        # We take the images and their labels and push them on GPU\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        batch_size = images.shape[0]\n",
        "\n",
        "        # Reset gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Obtain predictions\n",
        "        pred = model(images)\n",
        "\n",
        "        # Compute loss for this batch\n",
        "        loss = criterion(pred, labels)\n",
        "\n",
        "        # Compute gradients for each weight (backpropagation)\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights based on gradients (gradient descent)\n",
        "        optimizer.step()\n",
        "\n",
        "        # We keep track of the average training loss\n",
        "        total_train_loss += (loss.item() * batch_size)\n",
        "        dataset_size += batch_size\n",
        "\n",
        "        epoch_loss = np.round(total_train_loss / dataset_size, 2)\n",
        "        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss)\n",
        "\n",
        "    return epoch_loss"
      ],
      "metadata": {
        "id": "BX4VJbT3lQS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define the validation function in which the model is tested on unseen images.\n",
        "\n",
        "The validation function looks similar to the training one, except we do not compute gradients or do any gradient descent.\n",
        "\n",
        "Ideally, we should evaluate the model every few training epochs to see how the model generalizes to new examples."
      ],
      "metadata": {
        "id": "2VBlgB8SmxIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def valid_epoch(model, dataloader, device, criterion, epoch):\n",
        "    # We set the model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    total_val_loss = 0.0\n",
        "    dataset_size = 0\n",
        "\n",
        "    # We keep track of correct predictions\n",
        "    correct = 0\n",
        "\n",
        "    # This is only for showing the progress bar\n",
        "    bar = tqdm(enumerate(dataloader), total=len(dataloader), colour='cyan', file=sys.stdout)\n",
        "\n",
        "    for step, (images, labels) in bar:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        batch_size = images.shape[0]\n",
        "\n",
        "        pred = model(images)\n",
        "        loss = criterion(pred, labels)\n",
        "\n",
        "        # The raw output of the model is a score for each class\n",
        "        # We keep the index of the class with the highest score as the prediction\n",
        "        _, predicted = torch.max(pred, 1)\n",
        "\n",
        "        # We see how many predictions match the ground truth labels\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # We compute evaluation metrics - loss and accurarcy\n",
        "        total_val_loss += (loss.item() * batch_size)\n",
        "        dataset_size += batch_size\n",
        "\n",
        "        epoch_loss = np.round(total_val_loss / dataset_size, 2)\n",
        "\n",
        "        accuracy = np.round(100 * correct / dataset_size, 2)\n",
        "\n",
        "        bar.set_postfix(Epoch=epoch, Valid_Acc=accuracy, Valid_Loss=epoch_loss)\n",
        "\n",
        "    return accuracy, epoch_loss"
      ],
      "metadata": {
        "id": "oLrlitpknOnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define the function that receives the model and trains it with an optimizer and loss function (criterion) for a number of epochs."
      ],
      "metadata": {
        "id": "EH1WEeG-oDIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_training(model, criterion, optimizer, num_epochs):\n",
        "    # Check if we are using GPU\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n",
        "\n",
        "    # For keeping track of the best validation accuracy\n",
        "    top_accuracy = 0.0\n",
        "\n",
        "    # We train the emodel for a number of epochs\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        train_loss = train_epoch(model, trainloader, device, optimizer, criterion, epoch)\n",
        "\n",
        "        # For validation we do not keep track of gradients\n",
        "        with torch.no_grad():\n",
        "            val_accuracy, val_loss = valid_epoch(model, testloader, device, criterion, epoch)\n",
        "            if val_accuracy > top_accuracy:\n",
        "                print(f\"Validation Accuracy Improved ({top_accuracy} ---> {val_accuracy})\")\n",
        "                top_accuracy = val_accuracy\n",
        "        print()"
      ],
      "metadata": {
        "id": "Oi6sORPJoGBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fully Connected Neural Network"
      ],
      "metadata": {
        "id": "YehGuLAzojlf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first train a fully connected network (which we played with in Lab 1) on the images to compare it with CNN models.\n",
        "\n",
        "The fully connected neural network is not a very wise choice for images because we lose spatial information."
      ],
      "metadata": {
        "id": "OtFKC5GbonBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FCNNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FCNNet, self).__init__()\n",
        "\n",
        "        # Linear layers\n",
        "        # in_features of first layer should correspond to size of images - 3 channels, height 32, width 32\n",
        "        # out_features of last layer should correspond to number of classes - 10\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features=3 * 32 * 32, out_features=360)\n",
        "        self.fc2 = nn.Linear(in_features=360, out_features=120)\n",
        "        self.fc3 = nn.Linear(in_features=120, out_features=84)\n",
        "        self.fc4 = nn.Linear(in_features=84, out_features=10)\n",
        "\n",
        "        # Activation function - can be replaced by other better functions (GELU, Mish etc.)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # We transform the image into a 1D vector of size 3 * 32 * 32\n",
        "        x = x.view(-1, 3 * 32 * 32)\n",
        "\n",
        "        # Remember - we use activation functions after each linear layer (except the last one)\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.activation(self.fc2(x))\n",
        "        x = self.activation(self.fc3(x))\n",
        "\n",
        "        # Output is 10 scores, one for each class\n",
        "        x = self.fc4(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "zZOsdw1lo1Dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We construct the model and push it on GPU"
      ],
      "metadata": {
        "id": "D4Jmu8N1phqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FCNNet()\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "UBRGMhwhpfH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define the training helpers - hyperparameters, loss, optimizer"
      ],
      "metadata": {
        "id": "bDt8W5hrpU2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "epochs = 10\n",
        "\n",
        "# Loss - dependinng on the task you should change this\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Adam is an improved gradient descent algorithm\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "wK1ZV_qnpazF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We run the full training"
      ],
      "metadata": {
        "id": "9zy8Lthbpuan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_training(model, criterion, optimizer, epochs)"
      ],
      "metadata": {
        "id": "WBvkM6YxpvwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Convolutional Neural Network"
      ],
      "metadata": {
        "id": "ViHRRHxVqOwh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will construct convolutional neural network. We use:\n",
        "- [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) to define a convolutional layer. It has the following important arguments:\n",
        "  - in_channels - number of channels in input (for images it should be 3)\n",
        "  - out_channels - the number of feature maps to be produced by the layer - how many features you want to extract\n",
        "  - **in_channels of a inner layer should correspond to out_channels of previous layer**\n",
        "  - kernel_size - the size of the matrix of weights that slides across the input\n",
        "  - There are [other arguments]((https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) that you will get familiar with in the future\n",
        "- [MaxPool2D](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) to define a Max Pooling layer\n",
        "- [Activation functions](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)\n"
      ],
      "metadata": {
        "id": "lDr8m9hhqSz8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will build a CNN that has:\n",
        "- 2 convolutional layers with kernels of size 5x5\n",
        "- Max Pooling - to downsample spatial dimensions - you don't have to use it after each convolutional layer\n",
        "- A fully connected network that will\n",
        "  - Take as input the features that were extracted by the convolutional layers\n",
        "  - Output a score for each class"
      ],
      "metadata": {
        "id": "Scy2UlFCrYK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "\n",
        "        # First conv layer\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5)\n",
        "\n",
        "        # Second conv layer\n",
        "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
        "\n",
        "        # Pooling layer - used after both layers of convolution\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Linear layers of the fully connected classifier\n",
        "        self.fc1 = nn.Linear(in_features=16 * 5 * 5, out_features=120)\n",
        "        self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
        "        self.fc3 = nn.Linear(in_features=84, out_features=10)\n",
        "\n",
        "        # Activation function - can be replaced by other better functions (GELU, Mish etc.)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### Convolutional Part\n",
        "\n",
        "        # input - x shape - (3,32,32)\n",
        "\n",
        "        x = self.activation(self.conv1(x))\n",
        "        # After conv - x shape - (6,28,28)\n",
        "\n",
        "        x = self.pool(x)\n",
        "        # After pooling - x shape - (6,14,14)\n",
        "\n",
        "        x = self.activation(self.conv2(x))\n",
        "        # After conv - x shape - (16,10,10)\n",
        "\n",
        "        x = self.pool(x)\n",
        "        # After pooling - x shape - (16,5,5)\n",
        "\n",
        "        # Flatten features for input to fully connected layers - Transform the features into 1D vector\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "\n",
        "        ### Fully connected part\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.activation(self.fc2(x))\n",
        "\n",
        "        # Output is 10 scores, one for each class\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "mQkMCx9nq9Oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how the shape of the input **x** changes through convolutional layers and pooling layers. Convolution modifies the channel dimension based on the `out_channels` argument and the spatial dimension based on the dimension of the `kernel_size`. The spatial dimension of the output can also be modified based on the `padding` and `stride` arguments.\n",
        "\n",
        "The spatial dimension is computed with this formula: $[(W−K+2P)/S]+1$.\n",
        "\n",
        "* W is the input size - 32 (in first conv layer)\n",
        "* K is the Kernel size - 5\n",
        "* P is the padding - default 0\n",
        "* S is the stride - default 1\n",
        "\n",
        "You can check the full formula [here](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) in the 'Shape' section.\n",
        "\n",
        "It is important to monitor the shape of the input after each operation to make sure everything is working as expected.\n",
        "\n",
        "It is also important because you need to know the number of input neurons for the fully connected classifier."
      ],
      "metadata": {
        "id": "dhBn1TbDrHnm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercises"
      ],
      "metadata": {
        "id": "tTe2j0Ibu2dK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 1** - Run the training. Play with hyperparameters and network structure and try to improve results\n",
        "\n",
        "Things you may want to experiment with:\n",
        "- Changing the learning rate and number of epochs\n",
        "- Adding or removing linear layers in the fully connected part\n",
        "- Adding convolutional layers\n",
        "- Changing the kernel size and the number of output channels of the layers\n",
        "\n",
        "If you add convolutional layers be careful about adding pooling layers - they may downsample the spatial size too much and you will lose important information"
      ],
      "metadata": {
        "id": "e8kz5n8cu6wP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ConvNet()\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "VE4FSAUHvkkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "epochs = 10\n",
        "\n",
        "# Loss - dependinng on the task you should change this\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Adam is an improved gradient descent algorithm\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "vj7LkghUvmbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_training(model, criterion, optimizer, epochs)"
      ],
      "metadata": {
        "id": "ZAqmkE34vwer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 2** - Image Augmentations\n",
        "\n",
        "Your job is to add image augmentations to the training samples in order to improve the perfrmance of your CNN.\n",
        "\n",
        "Go to the cell in which we defined the trainset and trainloader and add\n",
        "\n",
        "**Explanation of Augmentations**\n",
        "\n",
        "Augmentations are operations applied to the input that modify it but keep its label. They are used for diversifying the training data.\n",
        "\n",
        "What operations can we apply to an image and not alter its class label?\n",
        "Examples: random rotations, random crops, random flips, etc.\n",
        "\n",
        "Luckily we do not have to implement these. They are already available in PyTorch.\n",
        "\n",
        "[Here](https://pytorch.org/vision/stable/auto_examples/transforms/plot_transforms_illustrations.html#sphx-glr-auto-examples-transforms-plot-transforms-illustrations-py) is an illustration of usual image transforms (augmentations).\n",
        "\n",
        "[Here](https://pytorch.org/vision/0.9/transforms.html) is a list of transforms you can use.\n"
      ],
      "metadata": {
        "id": "aEBAc2vtwtja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 3** - Chest X-ray Classification\n",
        "\n",
        "You will have to train a CNN to classify Chest X-ray images into 2 classes: Normal and Pneumonia.\n",
        "\n",
        "You will construct your own CNN model and train it.\n",
        "\n",
        "You can re-use the train_epoch, val_epoch, and run_training functions without modifying them."
      ],
      "metadata": {
        "id": "31GNztAA0krW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading data"
      ],
      "metadata": {
        "id": "8HU-QH-vAXtH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 'https://drive.google.com/uc?id=1-FUdrWc6hLTiLer_5Aj1OFsXcSxQXWiJ'\n",
        "!unzip -q xray_dataset.zip"
      ],
      "metadata": {
        "id": "nOe-YhQG2Y9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Necessary imports for reading the images"
      ],
      "metadata": {
        "id": "sM0zpl1sAbV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import PIL"
      ],
      "metadata": {
        "id": "J3_4wuao9AG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class for custom pytorch dataset. It is a wrapper for the code that reads images and returns samples + labels.\n",
        "\n",
        "It is used in conjunction with Dataloader which constructs batches of samples."
      ],
      "metadata": {
        "id": "K7sGbXhsAe9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ChestXrayDataset(Dataset):\n",
        "    def __init__(self, root_dir, train, transform=None):\n",
        "        if train:\n",
        "            print(\"Training dataset\")\n",
        "            self.data_path = os.path.join(root_dir, 'train')\n",
        "        else:\n",
        "            print(\"Test dataset\")\n",
        "            self.data_path = os.path.join(root_dir, 'test')\n",
        "\n",
        "        # Get all image paths\n",
        "        self.image_paths = [os.path.join(self.data_path, 'NORMAL', x) for x in os.listdir(os.path.join(self.data_path, 'NORMAL'))]\n",
        "        self.image_paths += [os.path.join(self.data_path, 'PNEUMONIA', x) for x in os.listdir(os.path.join(self.data_path, 'PNEUMONIA'))]\n",
        "\n",
        "        # Get labels\n",
        "        self.labels = [0]*len(os.listdir(os.path.join(self.data_path, 'NORMAL'))) + [1]*len(os.listdir(os.path.join(self.data_path, 'PNEUMONIA')))\n",
        "\n",
        "        # Print number of normal and pneumonia images\n",
        "        print(f'Normal Images: {len(os.listdir(os.path.join(self.data_path, \"NORMAL\")))}')\n",
        "        print(f'Pneumonia Images: {len(os.listdir(os.path.join(self.data_path, \"PNEUMONIA\")))}')\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = PIL.Image.open(self.image_paths[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n"
      ],
      "metadata": {
        "id": "qExXG6w22pKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create the dataloaders. As before, use train transforms for image augmentations."
      ],
      "metadata": {
        "id": "4E4j7bTYBQaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose(\n",
        "    [\n",
        "      # Add image transforms here\n",
        "\n",
        "\n",
        "      # I recommend resizing the images to 64x64 to have faster training\n",
        "      # If you increase it you may obtain better accuracy but training will take more\n",
        "      # If you change this, also change it in test_transforms\n",
        "      transforms.Resize((64, 64)),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "test_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((64, 64)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "trainset = ChestXrayDataset(root_dir='./chest_xray/', train=True, transform=train_transform)\n",
        "\n",
        "# The batch size is the number of images the model processes in parallel\n",
        "# We use shuffle for training as we don't want the model to see the images in the same order\n",
        "trainloader = DataLoader(trainset, batch_size=128,\n",
        "                                          shuffle=True)\n",
        "\n",
        "testset = ChestXrayDataset(root_dir='./chest_xray/', train=False, transform=test_transform)\n",
        "\n",
        "# For testing we don't have to shuffle the data\n",
        "testloader = DataLoader(testset, batch_size=128,\n",
        "                                         shuffle=False)\n",
        "\n",
        "classes = ('Normal', 'Pneumonia')"
      ],
      "metadata": {
        "id": "djl5N9HW9Ked"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show samples from the dataset. Re-run this cell to obtain different samples and see what you can observe"
      ],
      "metadata": {
        "id": "giGJTkx0BZbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images[:4]))\n",
        "\n",
        "# print labels\n",
        "print(' '.join('%14s' % classes[labels[j]] for j in range(4)))"
      ],
      "metadata": {
        "id": "UzOnz0Ez-T0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You have to construct the CNN and experiment with different configurations so that you obtain the best performance."
      ],
      "metadata": {
        "id": "O94OypBNBgQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ChestXrayConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ChestXrayConvNet, self).__init__()\n",
        "\n",
        "        # TODO - write conv layers and pooling\n",
        "\n",
        "        # TODO - write linear layers\n",
        "\n",
        "        # TODO - Activation function - Do not forget to use it in forward function after conv and linear layers\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### Convolutional Part\n",
        "\n",
        "        # TODO - write the pass through conv layers\n",
        "\n",
        "        # TODO - Flatten features for input to fully connected layers - Transform the features into 1D vector\n",
        "\n",
        "        ### Fully connected part\n",
        "        # TODO - write the pass through fully connected layers\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "WoHeOjpzBhmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChestXrayConvNet()\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "uCgVVQfMCQKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Choose Hyperparameters - these ones may not be good\n",
        "learning_rate = 0.001\n",
        "epochs = 10\n",
        "\n",
        "# Loss - dependinng on the task you should change this\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Adam is an improved gradient descent algorithm\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "Y6UWE3NRCZi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_training(model, criterion, optimizer, epochs)"
      ],
      "metadata": {
        "id": "soRgVFRXCszi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Problems\n",
        "\n",
        "In Deep Learning we are usually confronted with imbalanced datasets where some classes appear more frequently than others.\n",
        "- Notice that Pneumonia Images appear more frequently than normal ones in both training and testing\n",
        "- Accuracy may not be a good metric in this case. What accuracy will have a model that only predicts pneumonia? What metric should we use?\n",
        "- The model will see 3 times more training samples with pneumonia. It may learn to predict that pneumonia is more likely without actually looking for relevant cues in the image.\n",
        "\n",
        "Research what evaluation metrics to use for classification when the testing set is imbalanced.\n",
        "\n",
        "Research how we can improve the training when the training data is imbalanced."
      ],
      "metadata": {
        "id": "l4yqB6BNC9r9"
      }
    }
  ]
}